import pandas as pd
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    cohen_kappa_score,
    precision_score,
    recall_score,
    fbeta_score,
)

# ============================================================
# Load developer dataset
# ============================================================
PATH_VENDOR = "./Marque_ai/mq_ai_lite_results.csv"
df_v = pd.read_csv(PATH_VENDOR)

# ============================================================
# Helper: normalize Pass/Fail columns to 0/1
# ============================================================

def normalize_pass_fail(series):
    """
    Map various pass/fail string values to 1/0.
    Assumes developer uses 'Pass' / 'Fail'.
    """
    s = series.astype(str).str.strip().str.lower()
    mapping = {
        "pass": 1,
        "fail": 0,
        "1": 1,
        "0": 0,
        "true": 1,
        "false": 0,
        "yes": 1,
        "no": 0,
    }
    return s.map(mapping).astype(int)

# Column names from their file (based on your screenshot)
COL_LLM   = "LLM Eval (Show Answer to User)"
COL_HUMAN = "Human Eval (Show Answer to User)"

df_v["llm_pass"]   = normalize_pass_fail(df_v[COL_LLM])
df_v["human_pass"] = normalize_pass_fail(df_v[COL_HUMAN])

y_true = df_v["human_pass"]
y_llm  = df_v["llm_pass"]

# ============================================================
# (2) DIRECT AGREEMENT BETWEEN MODEL AND HUMAN EVALUATORS
# ============================================================

# Overall agreement rate
agreement_rate = (y_true == y_llm).mean()

# Confusion matrix (rows = human, cols = GPT-4.1)
cm = confusion_matrix(y_true, y_llm, labels=[0, 1])

# Cohen's kappa
kappa = cohen_kappa_score(y_true, y_llm)

print("=== Developer Dataset: Judge vs Human Agreement ===")
print(f"Number of samples: {len(df_v)}")
print(f"Raw agreement rate: {agreement_rate:.3f}")
print(f"Cohen's kappa:      {kappa:.3f}\n")

print("Confusion matrix (rows = Human, cols = GPT-4.1):")
print("      GPT-4.1:  Fail   Pass")
print("Human:")
print(f"  Fail      {cm[0,0]:5d} {cm[0,1]:5d}")
print(f"  Pass      {cm[1,0]:5d} {cm[1,1]:5d}\n")

print("Classification report (positive class = PASS):")
print(classification_report(
    y_true,
    y_llm,
    target_names=["Fail (0)", "Pass (1)"],
    digits=3
))

# ============================================================
# (3) END-TO-END PRECISION & RECALL
#     Scenario A: without judge (everything shown)
#     Scenario B: with GPT-4.1 judge
# ============================================================

# Scenario A: NO JUDGE — all answers are shown → predict PASS for all
y_pred_no_judge = pd.Series(1, index=df_v.index)

prec_no = precision_score(y_true, y_pred_no_judge, zero_division=0)
rec_no  = recall_score(y_true, y_pred_no_judge,  zero_division=0)
f05_no  = fbeta_score(y_true, y_pred_no_judge, beta=0.5, zero_division=0)

# Scenario B: WITH GPT-4.1 judge
prec_llm = precision_score(y_true, y_llm, zero_division=0)
rec_llm  = recall_score(y_true, y_llm,  zero_division=0)
f05_llm  = fbeta_score(y_true, y_llm, beta=0.5, zero_division=0)

print("=== End-to-End Performance (Human Eval as Ground Truth) ===")
print("Positive class = PASS (legitimate / grounded answer)\n")

print("Scenario A – Without judge (all answers shown):")
print(f"  Precision: {prec_no:.3f}")
print(f"  Recall:    {rec_no:.3f}")
print(f"  F0.5:      {f05_no:.3f}\n")

print("Scenario B – With GPT-4.1 judge:")
print(f"  Precision: {prec_llm:.3f}")
print(f"  Recall:    {rec_llm:.3f}")
print(f"  F0.5:      {f05_llm:.3f}")
